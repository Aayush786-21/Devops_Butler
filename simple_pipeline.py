"""
DevOps Butler - Unified Deployment Pipeline
Clean, reliable deployment pipeline without Nginx proxy dependencies.
Supports Docker Compose, Dockerfile, and AI-generated Dockerfile deployments.
"""

import subprocess
import os
import shutil
import uuid
import asyncio
import tempfile
import re
import socket
import yaml
from urllib.parse import urlparse
from typing import Optional, Tuple, Dict, Any
from pathlib import Path

# Import core modules
from docker_build import docker_build, cleanup_old_images
from docker_run import run_container
from connection_manager import manager
from sqlmodel import Session, select
from database import engine  
from login import Deployment

# Import AI capabilities (can be replaced with Gemini or removed)
# from ai_analyst import generate_dockerfile


def validate_git_url(git_url: str) -> bool:
    """Validates that the URL is a valid Git repository URL."""
    try:
        clean_url = git_url.rstrip('.git')
        
        # Check for invalid patterns (Docker registries, etc.)
        invalid_patterns = [
            'hub.docker.com', 'docker.io', 'quay.io', 
            'gcr.io', 'ecr.', 'azurecr.io'
        ]
        
        for pattern in invalid_patterns:
            if pattern in clean_url.lower():
                return False
        
        # Handle SSH URLs (git@github.com:user/repo)
        if clean_url.startswith('git@'):
            if ':' not in clean_url or clean_url.count(':') != 1:
                return False
            return True
        
        # Handle HTTPS URLs
        parsed = urlparse(clean_url)
        if not parsed.scheme or parsed.scheme not in ['http', 'https']:
            return False
        
        # Must have a path with at least 2 parts (user/repo)
        path_parts = parsed.path.strip('/').split('/')
        if len(path_parts) < 2:
            return False
        
        return True
    except Exception:
        return False


def extract_repo_name(git_url: str) -> str:
    """Extracts the repository name from a Git URL."""
    try:
        clean_url = git_url.rstrip('.git')
        
        if clean_url.startswith('git@'):
            # Handle SSH URLs
            repo_part = clean_url.split(':')[-1]
            parts = repo_part.split('/')
            if len(parts) >= 2:
                user_name = parts[-2]
                repo_name = parts[-1]
            else:
                user_name = "unknown"
                repo_name = parts[-1] if parts else "unknown"
        else:
            # Handle HTTPS URLs
            parsed = urlparse(clean_url)
            path_parts = parsed.path.strip('/').split('/')
            if len(path_parts) >= 2:
                user_name = path_parts[-2]
                repo_name = path_parts[-1]
            else:
                user_name = "unknown"
                repo_name = path_parts[-1] if path_parts else "unknown"
        
        # Clean up names (remove special characters)
        user_name = re.sub(r'[^a-zA-Z0-9-]', '-', user_name)
        repo_name = re.sub(r'[^a-zA-Z0-9-]', '-', repo_name)
        
        # Use user-repo combination for uniqueness
        unique_name = f"{user_name}-{repo_name}"
        
        # Ensure it starts with a letter or number
        if unique_name and not unique_name[0].isalnum():
            unique_name = 'repo-' + unique_name
        
        return unique_name.lower()
    except Exception as e:
        print(f"Error extracting repo name from {git_url}: {e}")
        return f"unknown-repo-{str(uuid.uuid4())[:8]}"


def find_exposed_port(dockerfile_path: str) -> Optional[int]:
    """Reads a Dockerfile and finds the first EXPOSE instruction."""
    try:
        with open(dockerfile_path, "r") as f:
            dockerfile_content = f.read()
            match = re.search(r'^\s*EXPOSE\s+(\d+)', dockerfile_content, re.MULTILINE | re.IGNORECASE)
            if match:
                port = int(match.group(1))
                print(f"Discovered EXPOSE port {port} from Dockerfile.")
                return port
        return None
    except FileNotFoundError:
        return None


def find_free_port(start_port=8080, max_port=65535) -> int:
    """Finds and returns a free port on the host."""
    for port in range(start_port, max_port):
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            if s.connect_ex(('localhost', port)) != 0:
                return port
    raise RuntimeError("No free ports available in range")


async def check_port_available(port: int) -> bool:
    """Check if a port is available."""
    try:
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            return s.connect_ex(('localhost', port)) != 0
    except Exception:
        return False


async def detect_running_port(container_name: str) -> Optional[int]:
    """Detects what port a running container is listening on by inspecting logs and processes."""
    try:
        # Check container logs for port information
        logs_result = await asyncio.to_thread(
            subprocess.run,
            ["docker", "logs", "--tail", "50", container_name],
            capture_output=True,
            text=True
        )
        
        if logs_result.returncode == 0:
            logs = logs_result.stdout + logs_result.stderr
            print(f"ğŸ“‹ Container logs for port detection (last 500 chars):\n{logs[-500:]}")
            
            # Common port detection patterns
            port_patterns = [
                r'listening on.*?:(\d+)',
                r'server.*?running.*?on.*?:(\d+)',
                r'Local:\s+http://localhost:(\d+)',
                r'âœ\s+Local:\s+http://localhost:(\d+)',
                r'listening.*?port\s+(\d+)',
                r'started.*?on.*?port\s+(\d+)',
                r'server.*?listening.*?(\d+)',
                r'running.*?on.*?port\s+(\d+)',
                r'Listening on port (\d+)',
                r'Server running on .*:(\d+)',
                r'listening on (\d+)',
            ]
            
            for pattern in port_patterns:
                matches = re.findall(pattern, logs, re.IGNORECASE)
                if matches:
                    # Filter out common non-port numbers
                    valid_ports = [int(m) for m in matches if 1024 <= int(m) <= 65535 or int(m) == 80]
                    if valid_ports:
                        detected_port = valid_ports[-1]  # Use the most recent match
                        print(f"ğŸ” Detected port {detected_port} from container logs")
                        return detected_port
        
        # Fallback: inspect container port mappings
        ports_inspect_result = await asyncio.to_thread(
            subprocess.run,
            ["docker", "inspect", container_name, "--format", "{{json .NetworkSettings.Ports}}"],
            capture_output=True,
            text=True
        )
        
        if ports_inspect_result.returncode == 0:
            import json
            try:
                ports_data = json.loads(ports_inspect_result.stdout.strip())
                print(f"ğŸ” Container port mappings: {ports_data}")
                
                # Look for exposed ports, prioritize common web ports
                port_priority = [80, 8080, 3000, 8000, 5000, 4000, 8888, 9000]
                found_ports = []
                
                for port_spec, mappings in ports_data.items():
                    if '/' in port_spec:
                        port_num = int(port_spec.split('/')[0])
                        found_ports.append(port_num)
                
                # Return the highest priority port if found
                for priority_port in port_priority:
                    if priority_port in found_ports:
                        print(f"ğŸ” Found priority port {priority_port} from container inspection")
                        return priority_port
                
                # If no priority port found, return the first exposed port
                if found_ports:
                    detected_port = found_ports[0]
                    print(f"ğŸ” Found exposed port {detected_port} from container inspection")
                    return detected_port
            except json.JSONDecodeError:
                print("âš ï¸ Could not parse container port mappings")
        
        return None
        
    except Exception as e:
        print(f"âš ï¸ Error detecting running port for {container_name}: {e}")
        return None


async def destroy_deployment(container_name: str):
    """Clean up a deployment by stopping and removing the container."""
    print(f"ğŸ—‘ï¸ Cleaning up deployment: {container_name}")
    
    try:
        # Stop the container
        await asyncio.to_thread(
            subprocess.run,
            ["docker", "stop", container_name],
            capture_output=True, text=True
        )
        
        # Remove the container
        await asyncio.to_thread(
            subprocess.run,
            ["docker", "rm", container_name],
            capture_output=True, text=True
        )
        
        print(f"âœ… Container {container_name} cleaned up")
    except Exception as e:
        print(f"âš ï¸ Error cleaning up {container_name}: {e}")


async def cleanup_conflicting_networks(repo_dir: str, compose_file_path: str):
    """Clean up conflicting Docker networks that might cause label issues."""
    try:
        # Read the compose file to find network names
        with open(compose_file_path, 'r') as f:
            compose_data = yaml.safe_load(f)
        
        networks = compose_data.get('networks', {})
        
        for network_name in networks.keys():
            try:
                # Try to remove the network if it exists
                result = await asyncio.to_thread(
                    subprocess.run,
                    ["docker", "network", "rm", network_name],
                    capture_output=True,
                    text=True
                )
                if result.returncode == 0:
                    print(f"ğŸ—‘ï¸ Removed conflicting network: {network_name}")
            except Exception:
                # Network might not exist or be in use, ignore
                pass
                
        # Also try to remove networks with common conflicting patterns
        conflicting_patterns = [
            f"{os.path.basename(repo_dir)}_default",
            "wanderlust",
            "wanderlust_default",
            "wanderlust_0"
        ]
        
        for pattern in conflicting_patterns:
            try:
                result = await asyncio.to_thread(
                    subprocess.run,
                    ["docker", "network", "rm", pattern],
                    capture_output=True,
                    text=True
                )
                if result.returncode == 0:
                    print(f"ğŸ—‘ï¸ Removed conflicting network pattern: {pattern}")
            except Exception:
                pass
                
    except Exception as e:
        print(f"âš ï¸ Warning during network cleanup: {e}")


async def handle_docker_compose_deployment(repo_dir: str, repo_name: str, container_name: str) -> Optional[str]:
    """Handle docker-compose deployment with simplified port mapping."""
    try:
        # Find compose file
        compose_files = ['docker-compose.yaml', 'docker-compose.yml']
        compose_file_path = None
        
        for compose_file in compose_files:
            potential_path = os.path.join(repo_dir, compose_file)
            if os.path.exists(potential_path):
                compose_file_path = potential_path
                break
        
        if not compose_file_path:
            return None
        
        await manager.broadcast("ğŸ³ Found docker-compose file, setting up multi-service deployment...")
        
        # First, stop and remove any existing containers from this compose file
        await manager.broadcast("ğŸ§¹ Cleaning up any existing containers and networks...")
        try:
            # Try to stop and remove containers from this compose project
            cleanup_result = await asyncio.to_thread(
                subprocess.run,
                ["docker-compose", "-f", compose_file_path, "down", "--remove-orphans", "-v", "--rmi", "local"],
                cwd=repo_dir,
                capture_output=True,
                text=True
            )
            if cleanup_result.returncode == 0:
                await manager.broadcast("âœ… Cleaned up existing containers and networks")
            else:
                print(f"âš ï¸ Cleanup warning (might be first deployment): {cleanup_result.stderr}")
                
            # Additional cleanup: remove conflicting networks manually
            await cleanup_conflicting_networks(repo_dir, compose_file_path)
            
        except Exception as e:
            print(f"âš ï¸ Cleanup warning: {e}")
        
        # Read and modify docker-compose file for localhost deployment
        with open(compose_file_path, 'r') as f:
            compose_data = yaml.safe_load(f)
        
        # Remove version if present (it's obsolete in newer compose versions)
        if 'version' in compose_data:
            del compose_data['version']
        
        # Update environment variables, ports, and container names
        services = compose_data.get('services', {})
        main_service_port = None
        unique_suffix = container_name.split('-')[-1]  # Get the UUID part
        
        for service_name, service_config in services.items():
            # Add unique container names to avoid conflicts
            service_config['container_name'] = f"{service_name}-{unique_suffix}"
            
            # Update environment variables to use localhost
            if 'environment' in service_config:
                env = service_config['environment']
                if isinstance(env, list):
                    for i, env_var in enumerate(env):
                        if isinstance(env_var, str):
                            # Replace IP addresses with localhost
                            env[i] = re.sub(r'http://\d+\.\d+\.\d+\.\d+:', 'http://localhost:', env_var)
                elif isinstance(env, dict):
                    for key, value in env.items():
                        if isinstance(value, str) and 'http://' in value:
                            env[key] = re.sub(r'http://\d+\.\d+\.\d+\.\d+:', 'http://localhost:', value)
            
            # Handle ports
            if 'ports' in service_config:
                ports = service_config['ports']
                for i, port_mapping in enumerate(ports):
                    if isinstance(port_mapping, str) and ':' in port_mapping:
                        host_port, container_port = port_mapping.split(':', 1)
                        try:
                            host_port_int = int(host_port)
                            # Check if port is available, if not find a free one
                            if not await check_port_available(host_port_int):
                                new_port = find_free_port(host_port_int + 1)
                                ports[i] = f"{new_port}:{container_port}"
                                print(f"âš ï¸ Port {host_port_int} busy, using {new_port} for {service_name}")
                                await manager.broadcast(f"âš ï¸ Port {host_port_int} busy, using {new_port} for {service_name}")
                            
                            # Track the main service port
                            if service_name in ['frontend', 'web', 'app'] or 'frontend' in service_name.lower():
                                main_service_port = int(ports[i].split(':')[0])
                        except ValueError:
                            continue
        
        # Handle networks - create unique network names or use external if specified
        if 'networks' in compose_data:
            networks = compose_data['networks']
            # Create a copy of the items to avoid "dictionary changed during iteration" error
            network_items = list(networks.items())
            networks_to_update = {}
            
            for network_name, network_config in network_items:
                if isinstance(network_config, dict) and not network_config.get('external', False):
                    # Make network name unique
                    new_network_name = f"{network_name}-{unique_suffix}"
                    
                    # Update services to use the new network name
                    for service_config in services.values():
                        if 'networks' in service_config:
                            if isinstance(service_config['networks'], list):
                                service_config['networks'] = [new_network_name if n == network_name else n 
                                                             for n in service_config['networks']]
                            elif isinstance(service_config['networks'], dict):
                                if network_name in service_config['networks']:
                                    service_config['networks'][new_network_name] = service_config['networks'].pop(network_name)
                    
                    # Store the updates to apply later
                    networks_to_update[new_network_name] = network_config
                    # Mark old network for removal
                    if network_name != new_network_name:
                        networks_to_update[network_name] = None
            
            # Apply the network updates
            for network_name, network_config in networks_to_update.items():
                if network_config is None:
                    # Remove old network
                    if network_name in compose_data['networks']:
                        del compose_data['networks'][network_name]
                else:
                    # Add/update new network
                    compose_data['networks'][network_name] = network_config
        
        # Write updated compose file
        with open(compose_file_path, 'w') as f:
            yaml.dump(compose_data, f, default_flow_style=False)
        
        await manager.broadcast("âš™ï¸ Starting multi-service deployment with docker-compose...")
        
        # Run docker-compose up
        compose_result = await asyncio.to_thread(
            subprocess.run,
            ["docker-compose", "-f", compose_file_path, "up", "-d"],
            cwd=repo_dir,
            capture_output=True,
            text=True
        )
        
        if compose_result.returncode != 0:
            print(f"âŒ Docker compose failed: {compose_result.stderr}")
            return None
        
        await manager.broadcast("âœ… Multi-service deployment successful!")
        
        # Return the main service port or the first available port
        if main_service_port:
            return f"http://localhost:{main_service_port}"
        else:
            # Find the first mapped port
            for service_name, service_config in services.items():
                if 'ports' in service_config and service_config['ports']:
                    port_mapping = service_config['ports'][0]
                    if isinstance(port_mapping, str) and ':' in port_mapping:
                        host_port = port_mapping.split(':')[0]
                        return f"http://localhost:{host_port}"
        
        return "http://localhost:8080"  # Fallback
    
    except Exception as e:
        print(f"âŒ Error in compose deployment: {e}")
        return None


async def handle_dockerfile_deployment(repo_dir: str, repo_name: str, container_name: str) -> Optional[str]:
    """Handle Dockerfile-based deployment."""
    try:
        await manager.broadcast(f"ğŸ“œ Building application from Dockerfile...")
        
        # Find the port from Dockerfile
        dockerfile_path = os.path.join(repo_dir, 'Dockerfile')
        internal_port = find_exposed_port(dockerfile_path)
        
        if internal_port:
            await manager.broadcast(f"âœ… Discovered EXPOSE port: {internal_port}")
        else:
            await manager.broadcast("â„¹ï¸ No EXPOSE port found, will detect after container starts")
            internal_port = 80  # Default assumption
        
        # Build the Docker image
        image_name = await docker_build(container_name, repo_dir)
        if not image_name:
            await manager.broadcast("âŒ Docker build failed")
            return None
        
        # Find a free port for direct access
        desired_port = find_free_port(internal_port if internal_port != 80 else 8080)
        
        # Run the container with port mapping
        run_success = await run_container(
            image_name, 
            container_name, 
            host_port=desired_port, 
            internal_port=internal_port
        )
        
        if not run_success:
            await manager.broadcast("âŒ Failed to start container")
            return None
        
        await manager.broadcast(f"âœ… Container started successfully")
        
        # Wait a moment and detect the actual running port
        await asyncio.sleep(3)
        detected_port = await detect_running_port(container_name)
        if detected_port and detected_port != internal_port:
            print(f"ğŸ¯ Detected actual running port: {detected_port}")
            internal_port = detected_port
        
        await manager.broadcast(f"ğŸŒ Application is accessible at: http://localhost:{desired_port}")
        return f"http://localhost:{desired_port}"
        
    except Exception as e:
        print(f"âŒ Error in Dockerfile deployment: {e}")
        return None


async def handle_static_website_deployment(repo_dir: str, repo_name: str, container_name: str) -> Optional[str]:
    """Handle static website deployment by generating a Dockerfile with Python HTTP server."""
    try:
        await manager.broadcast("ğŸŒ Generating Dockerfile for static website...")
        
        # Create the Python server script first
        server_script = '''import http.server
import socketserver
import os
from urllib.parse import unquote

class MyHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def end_headers(self):
        self.send_header("Cache-Control", "no-cache, no-store, must-revalidate")
        self.send_header("Pragma", "no-cache")
        self.send_header("Expires", "0")
        super().end_headers()

    def do_GET(self):
        # Handle SPA routing - serve index.html for routes
        if self.path != "/" and not os.path.exists(self.path.lstrip("/")) and not "." in os.path.basename(self.path):
            self.path = "/index.html"
        return super().do_GET()

PORT = int(os.environ.get("PORT", 8080))
with socketserver.TCPServer(("", PORT), MyHTTPRequestHandler) as httpd:
    print(f"Serving static files on port {PORT}")
    httpd.serve_forever()
'''
        
        # Write the server script to the repo directory
        server_script_path = os.path.join(repo_dir, 'server.py')
        with open(server_script_path, 'w') as f:
            f.write(server_script)
        
        # Create a simple Dockerfile
        dockerfile_content = '''FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Copy static files and server script
COPY . /app/

# Expose port 8080
EXPOSE 8080

# Start the Python server
CMD ["python", "server.py"]
'''
        
        # Write Dockerfile
        dockerfile_path = os.path.join(repo_dir, 'Dockerfile')
        with open(dockerfile_path, 'w') as f:
            f.write(dockerfile_content)
        
        await manager.broadcast("âœ… Generated Python-based Dockerfile and server script for static website")
        
        # Now deploy using the standard Dockerfile deployment
        return await handle_dockerfile_deployment(repo_dir, repo_name, container_name)
        
    except Exception as e:
        print(f"âŒ Error in static website deployment: {e}")
        return None


async def handle_ai_dockerfile_deployment(repo_dir: str, repo_name: str, container_name: str) -> Optional[str]:
    """Handle AI-generated Dockerfile deployment (placeholder - can be implemented with Gemini)."""
    try:
        await manager.broadcast("ğŸ¤– No Dockerfile found, would generate one with AI...")
        # Note: AI generation can be implemented here with Gemini API
        # For now, we'll return None to indicate this feature is not available
        
        await manager.broadcast("âš ï¸ AI Dockerfile generation not implemented - please provide a Dockerfile")
        return None
        
    except Exception as e:
        print(f"âŒ Error in AI dockerfile deployment: {e}")
        return None


async def run_deployment_pipeline(git_url: str, user_id: Optional[int] = None, env_dir: Optional[str] = None) -> Optional[Tuple[str, str]]:
    """
    Main deployment pipeline that handles the entire deployment process.
    
    Args:
        git_url: The Git repository URL to deploy
        user_id: The user ID for tracking
        env_dir: Optional directory containing environment files
    
    Returns:
        Tuple of (container_name, deployed_url) if successful, None if failed
    """
    final_status = "failed"
    server_url = None
    
    # Validate the Git URL
    if not validate_git_url(git_url):
        error_msg = f"âŒ Invalid Git repository URL: {git_url}"
        await manager.broadcast(error_msg)
        return None
    
    # Extract repository name and create container name
    repo_name = extract_repo_name(git_url)
    new_container_name = f"{repo_name}-{str(uuid.uuid4())[:8]}"
    
    print(f"ğŸš€ DEPLOYMENT PIPELINE: {new_container_name}")
    
    # Clean up old deployments
    with Session(engine) as session:
        old_deployment = session.exec(
            select(Deployment).where(Deployment.git_url == git_url, Deployment.status == 'success')
        ).first()
        
        if old_deployment:
            print(f"ğŸ” Found previous deployment: {old_deployment.container_name}")
            await destroy_deployment(old_deployment.container_name)
            await manager.broadcast("ğŸ§¹ Cleaned up previous deployment")
    
    # Create new deployment record
    with Session(engine) as session:
        db_deployment = Deployment(
            container_name=new_container_name,
            git_url=git_url,
            status="starting",
            user_id=user_id
        )
        session.add(db_deployment)
        session.commit()
        session.refresh(db_deployment)
    
    await manager.broadcast(f"ğŸ”µ Starting deployment - Container: {new_container_name}")
    
    # Create temporary workspace
    repo_dir = tempfile.mkdtemp(prefix="butler-deploy-")
    print(f"ğŸ“ Created workspace: {repo_dir}")
    
    try:
        # Clone repository
        await manager.broadcast("ğŸ“¥ Cloning repository...")
        command = ["git", "clone", "--depth", "1", str(git_url), repo_dir]
        
        try:
            await asyncio.to_thread(subprocess.run, command, check=True, capture_output=True, text=True)
            await manager.broadcast("âœ… Repository cloned successfully")
        except subprocess.CalledProcessError as e:
            error_msg = f"âŒ Failed to clone repository: {e.stderr}"
            await manager.broadcast(error_msg)
            with Session(engine) as session:
                deployment_to_update = session.get(Deployment, db_deployment.id)
                if deployment_to_update:
                    deployment_to_update.status = "failed"
                    session.add(deployment_to_update)
                    session.commit()
            return None
        
        # Copy env files if provided
        if env_dir:
            for env_file in ['frontend.env', 'backend.env']:
                src_path = os.path.join(env_dir, env_file)
                if os.path.exists(src_path):
                    dst_path = os.path.join(repo_dir, env_file)
                    shutil.copy2(src_path, dst_path)
                    await manager.broadcast(f"ğŸ“„ Environment file {env_file} copied")
        
        # Detect framework and create .dockerignore if needed
        await create_dockerignore_if_needed(repo_dir)
        
        # Deployment strategy selection
        compose_files = ['docker-compose.yaml', 'docker-compose.yml']
        dockerfile_names = ['Dockerfile', 'dockerfile', 'DOCKERFILE']
        
        has_compose = any(os.path.exists(os.path.join(repo_dir, f)) for f in compose_files)
        has_dockerfile = any(os.path.exists(os.path.join(repo_dir, f)) for f in dockerfile_names)
        
        # Debug: List repository contents
        try:
            repo_contents = os.listdir(repo_dir)
            await manager.broadcast(f"ğŸ“ Repository contents: {', '.join(repo_contents)}")
            print(f"ğŸ“ Repository contents: {repo_contents}")
        except Exception as e:
            print(f"âš ï¸ Could not list repository contents: {e}")
        
        # Debug: Log deployment strategy detection
        await manager.broadcast(f"ğŸ” Deployment strategy detection:")
        await manager.broadcast(f"   - Docker Compose files found: {has_compose}")
        await manager.broadcast(f"   - Dockerfile found: {has_dockerfile}")
        print(f"ğŸ” Strategy detection - Compose: {has_compose}, Dockerfile: {has_dockerfile}")
        
        # Strategy 1: Docker Compose (highest priority for multi-service)
        if has_compose:
            await manager.broadcast("ğŸ³ Docker-compose detected, deploying multi-service application...")
            print("ğŸ³ Using Docker Compose deployment strategy")
            server_url = await handle_docker_compose_deployment(repo_dir, repo_name, new_container_name)
            if server_url:
                final_status = "success"
                print(f"âœ… Docker Compose deployment successful: {server_url}")
            else:
                print("âŒ Docker Compose deployment failed")
        
        # Strategy 2: Dockerfile (single container)
        elif has_dockerfile:
            await manager.broadcast("ğŸ³ Dockerfile detected, deploying single container application...")
            print("ğŸ³ Using Dockerfile deployment strategy")
            
            # Normalize dockerfile name
            dockerfile_path = None
            for dockerfile_name in dockerfile_names:
                potential_path = os.path.join(repo_dir, dockerfile_name)
                if os.path.exists(potential_path):
                    dockerfile_path = potential_path
                    break
            
            if dockerfile_path and os.path.basename(dockerfile_path) != 'Dockerfile':
                normalized_dockerfile = os.path.join(repo_dir, 'Dockerfile')
                shutil.copy2(dockerfile_path, normalized_dockerfile)
                await manager.broadcast("âš™ï¸ Normalized dockerfile name for Docker compatibility")
            
            server_url = await handle_dockerfile_deployment(repo_dir, repo_name, new_container_name)
            if server_url:
                final_status = "success"
                print(f"âœ… Dockerfile deployment successful: {server_url}")
            else:
                print("âŒ Dockerfile deployment failed")
        
        # Strategy 3: Check for static website
        elif any(f.endswith('.html') for f in os.listdir(repo_dir) if os.path.isfile(os.path.join(repo_dir, f))):
            await manager.broadcast("ğŸŒ Static website detected, deploying with auto-generated Dockerfile...")
            print("ğŸŒ Using static website deployment strategy")
            server_url = await handle_static_website_deployment(repo_dir, repo_name, new_container_name)
            if server_url:
                final_status = "success"
                print(f"âœ… Static website deployment successful: {server_url}")
            else:
                print("âŒ Static website deployment failed")
        
        # Strategy 4: AI-generated Dockerfile (fallback)
        else:
            await manager.broadcast("ğŸ¤– No recognized deployment pattern, attempting AI-generated Dockerfile...")
            print("ğŸ¤– Using AI Dockerfile generation strategy")
            server_url = await handle_ai_dockerfile_deployment(repo_dir, repo_name, new_container_name)
            if server_url:
                final_status = "success"
                print(f"âœ… AI Dockerfile deployment successful: {server_url}")
            else:
                print("âŒ AI Dockerfile deployment failed")
        
        # Return results
        if final_status == "success" and server_url:
            await manager.broadcast(f"ğŸ‰ Deployment successful!")
            await manager.broadcast(f"ğŸŒ Access your app at: {server_url}")
            return new_container_name, server_url
        else:
            await manager.broadcast("âŒ Deployment failed")
            return None
    
    finally:
        # Update database with final status
        with Session(engine) as session:
            deployment_to_update = session.get(Deployment, db_deployment.id)
            if deployment_to_update:
                deployment_to_update.status = final_status
                if server_url:
                    deployment_to_update.deployed_url = server_url
                session.add(deployment_to_update)
                session.commit()
        
        # Clean up workspace
        if os.path.exists(repo_dir):
            print(f"ğŸ§¹ Cleaning up workspace: {repo_dir}")
            shutil.rmtree(repo_dir)
        
        # Clean up old Docker images
        try:
            print(f"ğŸ§¹ Cleaning up old images for {repo_name}...")
            await cleanup_old_images(repo_name, keep_count=3)
        except Exception as e:
            print(f"âš ï¸ Warning: Image cleanup failed: {e}")


async def create_dockerignore_if_needed(repo_dir: str):
    """Create .dockerignore for JavaScript/Node.js projects to prevent permission issues."""
    package_json_path = os.path.join(repo_dir, 'package.json')
    if os.path.exists(package_json_path):
        try:
            with open(package_json_path, 'r') as f:
                package_data = f.read()
                
                # Detect framework type
                if any(framework in package_data for framework in ['"next":', '"react":', '"vue":', '"angular":', '"svelte":']):
                    dockerignore_path = os.path.join(repo_dir, '.dockerignore')
                    if not os.path.exists(dockerignore_path):
                        await manager.broadcast("ğŸ“ Creating .dockerignore file for JavaScript compatibility")
                        js_dockerignore = """# Dependencies
node_modules/
npm-debug.log*
yarn-debug.log*
yarn-error.log*

# Build outputs and cache
.next/
out/
build/
dist/
.cache/

# Environment variables
.env*
!.env.example

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo

# OS files
.DS_Store
Thumbs.db

# Git
.git/
.gitignore
"""
                        with open(dockerignore_path, 'w') as f:
                            f.write(js_dockerignore)
                        await manager.broadcast("âœ… Created .dockerignore file for optimal build")
        except Exception as e:
            print(f"âš ï¸ Error creating .dockerignore: {e}")
